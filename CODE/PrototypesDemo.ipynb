{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["d7CphOaDxBg2","wSvyxrRMwyzX","sU32QPh7PaTT","hlOeYpBCg7xb","gqKvTqBROBr0"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ojfqL43Zit3","executionInfo":{"status":"ok","timestamp":1722961786898,"user_tz":-60,"elapsed":98809,"user":{"displayName":"MOHAMMED ALSIRAJI","userId":"11898263542769256225"}},"outputId":"b807bae4-e6c5-4272-fc3e-cadabdcdfec7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["#@title prepare for prediction\n","def prepareForSVMPrediction(frame_data , keypoint_columns):\n","  new_data = pd.DataFrame([frame_data])\n","\n","  for col in keypoint_columns:\n","      new_data[col + '_x'] = new_data[col].apply(lambda x: x[0])\n","      new_data[col + '_y'] = new_data[col].apply(lambda x: x[1])\n","  # Drop the original columns\n","  new_data.drop(columns=keypoint_columns, inplace=True)\n","  return new_data\n","def prepareForPrediction(frame_data , keypoint_columns):\n","  new_data = pd.DataFrame([frame_data])\n","  data = np.array([new_data.iloc[i].values.tolist() for i in range(len(new_data))])\n","  # Drop the original columns\n","  return data\n","\n"],"metadata":{"id":"qnoimAuGZy20","executionInfo":{"status":"ok","timestamp":1722961786900,"user_tz":-60,"elapsed":6,"user":{"displayName":"MOHAMMED ALSIRAJI","userId":"11898263542769256225"}},"cellView":"form"},"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d7CphOaDxBg2"},"source":["##MediaPipe\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22428,"status":"ok","timestamp":1722961809323,"user":{"displayName":"MOHAMMED ALSIRAJI","userId":"11898263542769256225"},"user_tz":-60},"id":"kl3p8nQ6wXRs","outputId":"9d22bca9-86e1-4bf4-d787-06764314b2c9","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["#@title Libraries\n","from google.colab import drive\n","drive.mount('/content/drive')\n","!pip install -q mediapipe\n","!wget -O pose_landmarker.task -q https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task\n","from mediapipe import solutions\n","from mediapipe.framework.formats import landmark_pb2\n","import numpy as np\n","import pandas as pd\n","import cv2\n","import os\n","from google.colab.patches import cv2_imshow\n","import mediapipe as mp\n","from mediapipe.tasks import python\n","from mediapipe.tasks.python import vision\n","import warnings\n","# Suppress specific warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning, module='google.protobuf.symbol_database')\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"bm0DOCWawqP9","cellView":"form","executionInfo":{"status":"ok","timestamp":1722961809323,"user_tz":-60,"elapsed":10,"user":{"displayName":"MOHAMMED ALSIRAJI","userId":"11898263542769256225"}}},"outputs":[],"source":["#@title Helper functions for visualization\n","\n","'''\n","0 - nose\n","1 - left eye (inner)\n","2 - left eye 1\n","3 - left eye (outer)\n","4 - right eye (inner)\n","5 - right eye 2\n","6 - right eye (outer)\n","7 - left ear 3\n","8 - right ear 4\n","9 - mouth (left)\n","10 - mouth (right)\n","11 - left shoulder 5\n","12 - right shoulder 6\n","13 - left elbow 7\n","14 - right elbow 8\n","15 - left wrist 9\n","16 - right wrist 10\n","17 - left pinky\n","18 - right pinky\n","19 - left index\n","20 - right index\n","21 - left thumb\n","22 - right thumb\n","23 - left hip 11\n","24 - right hip 12\n","25 - left knee 13\n","26 - right knee 14\n","27 - left ankle 15\n","28 - right ankle 16\n","29 - left heel\n","30 - right heel\n","31 - left foot index\n","32 - right foot index\n","'''\n","KEYPOINT_DICT = {\n","    'nose': 0,\n","    'left_eye': 1,\n","    'right_eye': 2,\n","    'left_ear': 3,\n","    'right_ear': 4,\n","    'left_shoulder': 5,\n","    'right_shoulder': 6,\n","    'left_elbow': 7,\n","    'right_elbow': 8,\n","    'left_wrist': 9,\n","    'right_wrist': 10,\n","    'left_hip': 11,\n","    'right_hip': 12,\n","    'left_knee': 13,\n","    'right_knee': 14,\n","    'left_ankle': 15,\n","    'right_ankle': 16\n","}\n","mpkeys = [0,2,5,7,8,11,12,13,14,15,16,23,24,25,26,27,28]\n","\n","def draw_landmarks_on_image(rgb_image, detection_result):\n","  pose_landmarks_list = detection_result.pose_landmarks\n","  annotated_image = np.copy(rgb_image)\n","  MPxy = []\n","  # Loop through the detected poses to visualize.\n","  for idx in range(len(pose_landmarks_list)):\n","    pose_landmarks = pose_landmarks_list[idx]\n","    # x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n","    # Draw the pose landmarks.\n","    pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n","    pose_landmarks_proto.landmark.extend([\n","      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n","    ])\n","    solutions.drawing_utils.draw_landmarks(\n","      annotated_image,\n","      pose_landmarks_proto,\n","      solutions.pose.POSE_CONNECTIONS,\n","      solutions.drawing_styles.get_default_pose_landmarks_style())\n","  #\n","    for i in range(17):\n","      MPxy.append((pose_landmarks[mpkeys[i]].x , pose_landmarks[mpkeys[i]].y))\n","\n","  # print('---------------------------','\\n landmark count:',len(pose_landmarks),'Example landmarkObj at 0 \\'nose\\': ',pose_landmarks[1],'Example x , y , z',pose_landmarks[1].x,pose_landmarks[1].y , pose_landmarks[1].z , '\\n--------------------------')\n","\n","  return annotated_image , MPxy"]},{"cell_type":"code","source":["#@title MediaPipe nn predict_video\n","MPkeypoint_columns = ['MediaPipe_nose',\n","       'MediaPipe_left_eye', 'MediaPipe_right_eye', 'MediaPipe_left_ear',\n","       'MediaPipe_right_ear', 'MediaPipe_left_shoulder',\n","       'MediaPipe_right_shoulder', 'MediaPipe_left_elbow',\n","       'MediaPipe_right_elbow', 'MediaPipe_left_wrist',\n","       'MediaPipe_right_wrist', 'MediaPipe_left_hip', 'MediaPipe_right_hip',\n","       'MediaPipe_left_knee', 'MediaPipe_right_knee', 'MediaPipe_left_ankle',\n","       'MediaPipe_right_ankle']\n","\n","def predict_Fall_MP(mp_svm_model,VIDEO_IN,vid_num,mtype):\n","  base_options = python.BaseOptions(model_asset_path='pose_landmarker.task')\n","  options = vision.PoseLandmarkerOptions(\n","      base_options=base_options,\n","      output_segmentation_masks=True,running_mode=vision.RunningMode.VIDEO)\n","  detector = vision.PoseLandmarker.create_from_options(options)\n","  cap = cv2.VideoCapture(VIDEO_IN)\n","  fps = cap.get(cv2.CAP_PROP_FPS)\n","\n","  time_step = 1.0 / fps\n","  preds = []\n","  # Get frame width and height\n","  frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","  frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","  vidoe_data = []\n","  out_path = os.path.join(OUT_directory, f'MediaPipe_{mtype}_detected_video_{vid_num}.mp4')\n","  out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*'XVID'), fps, (frame_width, frame_height))\n","\n","  num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","  fall_detected = False\n","  fall_time = None\n","\n","\n","  # Initialize a variable to keep track of the current timestamp\n","  current_timestamp = 0.0\n","  idx = 0\n","  bar = display(progress(0, num_frames-1), display_id=True)\n","\n","  # Loop through each frame in the video using VideoCapture#read()\n","  while cap.isOpened():\n","      ret, frame = cap.read()\n","      if not ret:\n","          break\n","\n","      # Convert the frame received from OpenCV to a MediaPipe’s Image object.\n","      # Note: MediaPipe's Image object expects the data to be in RGB format.\n","      mpstart_time = time.time()\n","\n","\n","      rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","      mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n","\n","      # You can process the mp_image_object further with MediaPipe solutions here.\n","      # Example: Use mp_image_object with a MediaPipe Face Detection model.\n","      timestamp_ms = current_timestamp * 1000\n","      timestamp_us = int(timestamp_ms * 1000)\n","      pose_landmarker_result = detector.detect_for_video(mp_image, timestamp_us)\n","      poses_count = len(pose_landmarker_result.pose_landmarks)\n","      annotated_image , MPxy = draw_landmarks_on_image(mp_image.numpy_view(), pose_landmarker_result)\n","      # cv2_imshow(annotated_image)\n","\n","      frame_data = {}\n","      for i in range(17):\n","        keypoint_name = list(KEYPOINT_DICT.keys())[list(KEYPOINT_DICT.values()).index(i)]\n","        if poses_count > 0:\n","          frame_data[f'MediaPipe_{keypoint_name}'] = [MPxy[i][0],MPxy[i][1]]\n","        else:\n","          frame_data[f'MediaPipe_{keypoint_name}'] = [0.00,0.00]\n","\n","      # Save the frame to the output video\n","\n","      prediction = mp_svm_model.predict(prepareForPrediction(frame_data , MPkeypoint_columns))\n","      # print('prediction:: ',prediction)\n","\n","      label = 'Fall' if prediction[0][0] > 0.7 else 'Not Fall'\n","      # print(prediction)\n","      preds.append(prediction[0][0])\n","      # label = 'PREDECTING'#predictions[0]\n","      cv2.putText(annotated_image,f'{label}: {(prediction[0][0]*100):.2f}%', (10, 30),\n","          cv2.FONT_HERSHEY_SIMPLEX, 1,(0, 255, 0) if label == 'Not Fall' else (0, 0, 255), 2)\n","\n","\n","      title_text = 'Pose by MediaPipe'\n","      text_size, _ = cv2.getTextSize(title_text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n","      text_width, text_height = text_size\n","\n","      cv2.putText(annotated_image, title_text, (frame_width - 350, 30),\n","          cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n","\n","\n","\n","      timestamp_text = f'Time: {current_timestamp:.2f}s'\n","      cv2.putText(annotated_image, timestamp_text,(30, frame_height - 20),\n","          cv2.FONT_HERSHEY_SIMPLEX, 1,(0, 255, 0) if label == 'Not Fall' else (0, 0, 255), 2)\n","\n","      if label == 'Fall' and not fall_detected:\n","          fall_time = current_timestamp\n","          fall_detected = True\n","\n","      # Display the fall detection time if a fall was detected\n","      if fall_detected:\n","          fall_time_text = f'Fall detected at: {fall_time:.2f} s'\n","          cv2.putText(annotated_image, fall_time_text, (10, 100),\n","                      cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n","\n","\n","      out.write(annotated_image)\n","\n","      mpend_time = time.time()\n","      mpexecution_time = (mpend_time - mpstart_time) * 1000  # convert to milliseconds\n","\n","      mp_vid = {\n","      'Video_num':vid_num,\n","      'frame_indx':idx ,\n","      'predictions':prediction[0][0],\n","      'frame_proccessing_duration':mpexecution_time,\n","      'fall_detection_duration_perVideo':fall_time ,\n","      'frame_width': frame_width,\n","      'frame_height': frame_height\n","      }\n","\n","      vidoe_data.append(mp_vid)\n","\n","      current_timestamp += time_step\n","      # print(f\"Frame: {idx} / {num_frames}\")\n","      bar.update(progress(idx, num_frames-1))\n","      idx += 1\n","      if cv2.waitKey(int(1000/fps)) & 0xFF == ord('q'):\n","          break\n","  vidoe_datadf = pd.DataFrame(vidoe_data)\n","  csv_path = os.path.join(OUT_directory,f'MediaPipe_{mtype}_results_vid{vid_num}.csv')\n","  vidoe_datadf.to_csv(csv_path, index=False)\n","\n","\n","  cap.release()\n","  out.release()\n","  cv2.destroyAllWindows()\n","  return preds , fall_time"],"metadata":{"cellView":"form","id":"n_53f7Oufd7h","executionInfo":{"status":"ok","timestamp":1722961809323,"user_tz":-60,"elapsed":8,"user":{"displayName":"MOHAMMED ALSIRAJI","userId":"11898263542769256225"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"id":"2Z3ALZoSwuVd","cellView":"form","executionInfo":{"status":"ok","timestamp":1722961809323,"user_tz":-60,"elapsed":8,"user":{"displayName":"MOHAMMED ALSIRAJI","userId":"11898263542769256225"}}},"outputs":[],"source":["#@title MediaPipe svm predict_video\n","MPkeypoint_columns = ['MediaPipe_nose',\n","       'MediaPipe_left_eye', 'MediaPipe_right_eye', 'MediaPipe_left_ear',\n","       'MediaPipe_right_ear', 'MediaPipe_left_shoulder',\n","       'MediaPipe_right_shoulder', 'MediaPipe_left_elbow',\n","       'MediaPipe_right_elbow', 'MediaPipe_left_wrist',\n","       'MediaPipe_right_wrist', 'MediaPipe_left_hip', 'MediaPipe_right_hip',\n","       'MediaPipe_left_knee', 'MediaPipe_right_knee', 'MediaPipe_left_ankle',\n","       'MediaPipe_right_ankle']\n","\n","def predict_Fall_SVM_MP(mp_svm_model,VIDEO_IN,vid_num):\n","  base_options = python.BaseOptions(model_asset_path='pose_landmarker.task')\n","  options = vision.PoseLandmarkerOptions(\n","      base_options=base_options,\n","      output_segmentation_masks=True,running_mode=vision.RunningMode.VIDEO)\n","  detector = vision.PoseLandmarker.create_from_options(options)\n","  cap = cv2.VideoCapture(VIDEO_IN)\n","  fps = cap.get(cv2.CAP_PROP_FPS)\n","\n","  time_step = 1.0 / fps\n","  preds = []\n","  # Get frame width and height\n","  frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","  frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","  vidoe_data = []\n","  out_path = os.path.join(OUT_directory, f'MediaPipe_SVM_detected_video_{vid_num}.mp4')\n","  out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*'XVID'), fps, (frame_width, frame_height))\n","\n","  num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","  fall_detected = False\n","  fall_time = None\n","\n","\n","  # Initialize a variable to keep track of the current timestamp\n","  current_timestamp = 0.0\n","  idx = 0\n","  bar = display(progress(0, num_frames-1), display_id=True)\n","\n","  # Loop through each frame in the video using VideoCapture#read()\n","  while cap.isOpened():\n","      ret, frame = cap.read()\n","      if not ret:\n","          break\n","\n","      # Convert the frame received from OpenCV to a MediaPipe’s Image object.\n","      # Note: MediaPipe's Image object expects the data to be in RGB format.\n","      mpstart_time = time.time()\n","\n","\n","      rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","      mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n","\n","      # You can process the mp_image_object further with MediaPipe solutions here.\n","      # Example: Use mp_image_object with a MediaPipe Face Detection model.\n","      timestamp_ms = current_timestamp * 1000\n","      timestamp_us = int(timestamp_ms * 1000)\n","      pose_landmarker_result = detector.detect_for_video(mp_image, timestamp_us)\n","      poses_count = len(pose_landmarker_result.pose_landmarks)\n","      annotated_image , MPxy = draw_landmarks_on_image(mp_image.numpy_view(), pose_landmarker_result)\n","      # cv2_imshow(annotated_image)\n","\n","      frame_data = {}\n","      for i in range(17):\n","        keypoint_name = list(KEYPOINT_DICT.keys())[list(KEYPOINT_DICT.values()).index(i)]\n","        if poses_count > 0:\n","          frame_data[f'MediaPipe_{keypoint_name}'] = [MPxy[i][0],MPxy[i][1]]\n","        else:\n","          frame_data[f'MediaPipe_{keypoint_name}'] = [0.00,0.00]\n","\n","      # Save the frame to the output video\n","\n","      prediction = mp_svm_model.predict(prepareForSVMPrediction(frame_data , MPkeypoint_columns))\n","      label = 'Fall' if prediction[0] == 1 else 'Not Fall'\n","      # print(prediction)\n","      preds.append(prediction[0])\n","      # label = 'PREDECTING'#predictions[0]\n","      cv2.putText(annotated_image,f'{label}: {prediction}', (10, 30),\n","          cv2.FONT_HERSHEY_SIMPLEX, 1,(0, 255, 0) if label == 'Not Fall' else (0, 0, 255), 2)\n","\n","\n","      title_text = 'Pose by MediaPipe'\n","      text_size, _ = cv2.getTextSize(title_text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n","      text_width, text_height = text_size\n","\n","      cv2.putText(annotated_image, title_text, (frame_width - 350, 30),\n","          cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n","\n","\n","\n","      timestamp_text = f'Time: {current_timestamp:.2f}s'\n","      cv2.putText(annotated_image, timestamp_text,(30, frame_height - 20),\n","          cv2.FONT_HERSHEY_SIMPLEX, 1,(0, 255, 0) if label == 'Not Fall' else (0, 0, 255), 2)\n","\n","      if label == 'Fall' and not fall_detected:\n","          fall_time = current_timestamp\n","          fall_detected = True\n","\n","      # Display the fall detection time if a fall was detected\n","      if fall_detected:\n","          fall_time_text = f'Fall detected at: {fall_time:.2f} s'\n","          cv2.putText(annotated_image, fall_time_text, (10, 100),\n","                      cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n","\n","\n","      out.write(annotated_image)\n","\n","      mpend_time = time.time()\n","      mpexecution_time = (mpend_time - mpstart_time) * 1000  # convert to milliseconds\n","\n","      mp_vid = {\n","      'Video_num':vid_num,\n","      'frame_indx':idx ,\n","      'predictions':prediction[0],\n","      'frame_proccessing_duration':mpexecution_time,\n","      'fall_detection_duration_perVideo':fall_time ,\n","      'frame_width': frame_width,\n","      'frame_height': frame_height\n","      }\n","\n","      vidoe_data.append(mp_vid)\n","\n","      current_timestamp += time_step\n","      # print(f\"Frame: {idx} / {num_frames}\")\n","      bar.update(progress(idx, num_frames-1))\n","      idx += 1\n","      if cv2.waitKey(int(1000/fps)) & 0xFF == ord('q'):\n","          break\n","  vidoe_datadf = pd.DataFrame(vidoe_data)\n","  csv_path = os.path.join(OUT_directory,f'MediaPipe_SVM_results_vid{vid_num}.csv')\n","  vidoe_datadf.to_csv(csv_path, index=False)\n","\n","\n","  cap.release()\n","  out.release()\n","  cv2.destroyAllWindows()\n","  return preds , fall_time"]},{"cell_type":"markdown","metadata":{"id":"wSvyxrRMwyzX"},"source":["##MoveNet"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17412,"status":"ok","timestamp":1722961826727,"user":{"displayName":"MOHAMMED ALSIRAJI","userId":"11898263542769256225"},"user_tz":-60},"id":"_xXS9bY4w9iz","outputId":"2929bf6c-68d5-4c6f-ccff-236271335c5c","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["#@title Libraries\n","!pip install -q git+https://github.com/tensorflow/docs\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from tensorflow_docs.vis import embed\n","import numpy as np\n","import cv2\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","import os\n","# Import matplotlib libraries\n","from matplotlib import pyplot as plt\n","from matplotlib.collections import LineCollection\n","import matplotlib.patches as patches\n","\n","# Some modules to display an animation using imageio.\n","import imageio\n","from IPython.display import HTML, display"]},{"cell_type":"code","execution_count":8,"metadata":{"cellView":"form","id":"DGSTul5pw_1X","executionInfo":{"status":"ok","timestamp":1722961856829,"user_tz":-60,"elapsed":30113,"user":{"displayName":"MOHAMMED ALSIRAJI","userId":"11898263542769256225"}}},"outputs":[],"source":["#@title Model\n","\n","model_name = \"movenet_lightning\" #@param [\"movenet_lightning\", \"movenet_thunder\", \"movenet_lightning_f16.tflite\", \"movenet_thunder_f16.tflite\", \"movenet_lightning_int8.tflite\", \"movenet_thunder_int8.tflite\"]\n","\n","if \"tflite\" in model_name:\n","  if \"movenet_lightning_f16\" in model_name:\n","    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4?lite-format=tflite\n","    input_size = 192\n","  elif \"movenet_thunder_f16\" in model_name:\n","    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite\n","    input_size = 256\n","  elif \"movenet_lightning_int8\" in model_name:\n","    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4?lite-format=tflite\n","    input_size = 192\n","  elif \"movenet_thunder_int8\" in model_name:\n","    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/int8/4?lite-format=tflite\n","    input_size = 256\n","  else:\n","    raise ValueError(\"Unsupported model name: %s\" % model_name)\n","\n","  # Initialize the TFLite interpreter\n","  interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n","  interpreter.allocate_tensors()\n","\n","  def movenet(input_image):\n","    \"\"\"Runs detection on an input image.\n","\n","    Args:\n","      input_image: A [1, height, width, 3] tensor represents the input image\n","        pixels. Note that the height/width should already be resized and match the\n","        expected input resolution of the model before passing into this function.\n","\n","    Returns:\n","      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n","      coordinates and scores.\n","    \"\"\"\n","    # TF Lite format expects tensor type of uint8.\n","    input_image = tf.cast(input_image, dtype=tf.uint8)\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n","    # Invoke inference.\n","    interpreter.invoke()\n","    # Get the model prediction.\n","    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n","    return keypoints_with_scores\n","\n","else:\n","  if \"movenet_lightning\" in model_name:\n","    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n","    input_size = 192\n","  elif \"movenet_thunder\" in model_name:\n","    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n","    input_size = 256\n","  else:\n","    raise ValueError(\"Unsupported model name: %s\" % model_name)\n","\n","  def movenet(input_image):\n","    \"\"\"Runs detection on an input image.\n","\n","    Args:\n","      input_image: A [1, height, width, 3] tensor represents the input image\n","        pixels. Note that the height/width should already be resized and match the\n","        expected input resolution of the model before passing into this function.\n","\n","    Returns:\n","      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n","      coordinates and scores.\n","    \"\"\"\n","    model = module.signatures['serving_default']\n","\n","    # SavedModel format expects tensor type of int32.\n","    input_image = tf.cast(input_image, dtype=tf.int32)\n","    # Run model inference.\n","    outputs = model(input_image)\n","    # Output is a [1, 1, 17, 3] tensor.\n","    keypoints_with_scores = outputs['output_0'].numpy()\n","    return keypoints_with_scores"]},{"cell_type":"code","execution_count":9,"metadata":{"cellView":"form","id":"CU9igYRLwxn8","executionInfo":{"status":"ok","timestamp":1722961856829,"user_tz":-60,"elapsed":9,"user":{"displayName":"MOHAMMED ALSIRAJI","userId":"11898263542769256225"}}},"outputs":[],"source":["#@title Helper functions for visualization\n","\n","# Dictionary that maps from joint names to keypoint indices.\n","KEYPOINT_DICT = {\n","    'nose': 0,\n","    'left_eye': 1,\n","    'right_eye': 2,\n","    'left_ear': 3,\n","    'right_ear': 4,\n","    'left_shoulder': 5,\n","    'right_shoulder': 6,\n","    'left_elbow': 7,\n","    'right_elbow': 8,\n","    'left_wrist': 9,\n","    'right_wrist': 10,\n","    'left_hip': 11,\n","    'right_hip': 12,\n","    'left_knee': 13,\n","    'right_knee': 14,\n","    'left_ankle': 15,\n","    'right_ankle': 16\n","}\n","\n","# Maps bones to a matplotlib color name.\n","KEYPOINT_EDGE_INDS_TO_COLOR = {\n","    (0, 1): 'm',\n","    (0, 2): 'c',\n","    (1, 3): 'm',\n","    (2, 4): 'c',\n","    (0, 5): 'm',\n","    (0, 6): 'c',\n","    (5, 7): 'm',\n","    (7, 9): 'm',\n","    (6, 8): 'c',\n","    (8, 10): 'c',\n","    (5, 6): 'y',\n","    (5, 11): 'm',\n","    (6, 12): 'c',\n","    (11, 12): 'y',\n","    (11, 13): 'm',\n","    (13, 15): 'm',\n","    (12, 14): 'c',\n","    (14, 16): 'c'\n","}\n","\n","def _keypoints_and_edges_for_display(keypoints_with_scores,\n","                                     height,\n","                                     width,\n","                                     keypoint_threshold=0.11):\n","  \"\"\"Returns high confidence keypoints and edges for visualization.\n","\n","  Args:\n","    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n","      the keypoint coordinates and scores returned from the MoveNet model.\n","    height: height of the image in pixels.\n","    width: width of the image in pixels.\n","    keypoint_threshold: minimum confidence score for a keypoint to be\n","      visualized.\n","\n","  Returns:\n","    A (keypoints_xy, edges_xy, edge_colors) containing:\n","      * the coordinates of all keypoints of all detected entities;\n","      * the coordinates of all skeleton edges of all detected entities;\n","      * the colors in which the edges should be plotted.\n","  \"\"\"\n","  keypoints_all = []\n","  keypoint_edges_all = []\n","  edge_colors = []\n","  num_instances, _, _, _ = keypoints_with_scores.shape\n","  for idx in range(num_instances):\n","    kpts_x = keypoints_with_scores[0, idx, :, 1]\n","    kpts_y = keypoints_with_scores[0, idx, :, 0]\n","    kpts_scores = keypoints_with_scores[0, idx, :, 2]\n","    kpts_absolute_xy = np.stack(\n","        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n","    kpts_above_thresh_absolute = kpts_absolute_xy[\n","        kpts_scores > keypoint_threshold, :]\n","    keypoints_all.append(kpts_above_thresh_absolute)\n","\n","    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n","      if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n","          kpts_scores[edge_pair[1]] > keypoint_threshold):\n","        x_start = kpts_absolute_xy[edge_pair[0], 0]\n","        y_start = kpts_absolute_xy[edge_pair[0], 1]\n","        x_end = kpts_absolute_xy[edge_pair[1], 0]\n","        y_end = kpts_absolute_xy[edge_pair[1], 1]\n","        line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n","        keypoint_edges_all.append(line_seg)\n","        edge_colors.append(color)\n","  if keypoints_all:\n","    keypoints_xy = np.concatenate(keypoints_all, axis=0)\n","  else:\n","    keypoints_xy = np.zeros((0, 17, 2))\n","\n","  if keypoint_edges_all:\n","    edges_xy = np.stack(keypoint_edges_all, axis=0)\n","  else:\n","    edges_xy = np.zeros((0, 2, 2))\n","  return keypoints_xy, edges_xy, edge_colors\n","\n","\n","def draw_prediction_on_image(\n","    image, keypoints_with_scores, crop_region=None, close_figure=False,\n","    output_image_height=None):\n","  \"\"\"Draws the keypoint predictions on image.\n","\n","  Args:\n","    image: A numpy array with shape [height, width, channel] representing the\n","      pixel values of the input image.\n","    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n","      the keypoint coordinates and scores returned from the MoveNet model.\n","    crop_region: A dictionary that defines the coordinates of the bounding box\n","      of the crop region in normalized coordinates (see the init_crop_region\n","      function below for more detail). If provided, this function will also\n","      draw the bounding box on the image.\n","    output_image_height: An integer indicating the height of the output image.\n","      Note that the image aspect ratio will be the same as the input image.\n","\n","  Returns:\n","    A numpy array with shape [out_height, out_width, channel] representing the\n","    image overlaid with keypoint predictions.\n","  \"\"\"\n","  height, width, channel = image.shape\n","  aspect_ratio = float(width) / height\n","  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n","  # To remove the huge white borders\n","  fig.tight_layout(pad=0)\n","  ax.margins(0)\n","  ax.set_yticklabels([])\n","  ax.set_xticklabels([])\n","  plt.axis('off')\n","\n","  im = ax.imshow(image)\n","  line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n","  ax.add_collection(line_segments)\n","  # Turn off tick labels\n","  scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n","\n","  (keypoint_locs, keypoint_edges,\n","   edge_colors) = _keypoints_and_edges_for_display(\n","       keypoints_with_scores, height, width)\n","\n","  line_segments.set_segments(keypoint_edges)\n","  line_segments.set_color(edge_colors)\n","  if keypoint_edges.shape[0]:\n","    line_segments.set_segments(keypoint_edges)\n","    line_segments.set_color(edge_colors)\n","  if keypoint_locs.shape[0]:\n","    scat.set_offsets(keypoint_locs)\n","\n","  if crop_region is not None:\n","    xmin = max(crop_region['x_min'] * width, 0.0)\n","    ymin = max(crop_region['y_min'] * height, 0.0)\n","    rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n","    rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n","    rect = patches.Rectangle(\n","        (xmin,ymin),rec_width,rec_height,\n","        linewidth=1,edgecolor='b',facecolor='none')\n","    ax.add_patch(rect)\n","\n","  fig.canvas.draw()\n","  image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n","  image_from_plot = image_from_plot.reshape(\n","      fig.canvas.get_width_height()[::-1] + (3,))\n","  plt.close(fig)\n","  if output_image_height is not None:\n","    output_image_width = int(output_image_height / height * width)\n","    image_from_plot = cv2.resize(\n","        image_from_plot, dsize=(output_image_width, output_image_height),\n","         interpolation=cv2.INTER_CUBIC)\n","  MNxy = []\n","  for i in range(17):\n","    kpts_x = keypoints_with_scores[:, :, i, 1][0][0]\n","    kpts_y = keypoints_with_scores[:, :, i, 0][0][0]\n","    MNxy.append((kpts_x,kpts_y))\n","\n","\n","  return image_from_plot\n","\n","def to_gif(images, duration):\n","  \"\"\"Converts image sequence (4D numpy array) to gif.\"\"\"\n","  imageio.mimsave('./animation.gif', images, duration=duration)\n","  return embed.embed_file('./animation.gif')\n","\n","def progress(value, max=100):\n","  return HTML(\"\"\"\n","      <progress\n","          value='{value}'\n","          max='{max}',\n","          style='width: 100%'\n","      >\n","          {value}\n","      </progress>\n","  \"\"\".format(value=value, max=max))"]},{"cell_type":"code","execution_count":10,"metadata":{"cellView":"form","id":"xlykbbwlxsOT","executionInfo":{"status":"ok","timestamp":1722961856830,"user_tz":-60,"elapsed":8,"user":{"displayName":"MOHAMMED ALSIRAJI","userId":"11898263542769256225"}}},"outputs":[],"source":["#@title Cropping Algorithm\n","\n","# Confidence score to determine whether a keypoint prediction is reliable.\n","MIN_CROP_KEYPOINT_SCORE = 0.2\n","\n","def init_crop_region(image_height, image_width):\n","  \"\"\"Defines the default crop region.\n","\n","  The function provides the initial crop region (pads the full image from both\n","  sides to make it a square image) when the algorithm cannot reliably determine\n","  the crop region from the previous frame.\n","  \"\"\"\n","  if image_width > image_height:\n","    box_height = image_width / image_height\n","    box_width = 1.0\n","    y_min = (image_height / 2 - image_width / 2) / image_height\n","    x_min = 0.0\n","  else:\n","    box_height = 1.0\n","    box_width = image_height / image_width\n","    y_min = 0.0\n","    x_min = (image_width / 2 - image_height / 2) / image_width\n","\n","  return {\n","    'y_min': y_min,\n","    'x_min': x_min,\n","    'y_max': y_min + box_height,\n","    'x_max': x_min + box_width,\n","    'height': box_height,\n","    'width': box_width\n","  }\n","\n","def torso_visible(keypoints):\n","  \"\"\"Checks whether there are enough torso keypoints.\n","\n","  This function checks whether the model is confident at predicting one of the\n","  shoulders/hips which is required to determine a good crop region.\n","  \"\"\"\n","  return ((keypoints[0, 0, KEYPOINT_DICT['left_hip'], 2] >\n","           MIN_CROP_KEYPOINT_SCORE or\n","          keypoints[0, 0, KEYPOINT_DICT['right_hip'], 2] >\n","           MIN_CROP_KEYPOINT_SCORE) and\n","          (keypoints[0, 0, KEYPOINT_DICT['left_shoulder'], 2] >\n","           MIN_CROP_KEYPOINT_SCORE or\n","          keypoints[0, 0, KEYPOINT_DICT['right_shoulder'], 2] >\n","           MIN_CROP_KEYPOINT_SCORE))\n","\n","def determine_torso_and_body_range(\n","    keypoints, target_keypoints, center_y, center_x):\n","  \"\"\"Calculates the maximum distance from each keypoints to the center location.\n","\n","  The function returns the maximum distances from the two sets of keypoints:\n","  full 17 keypoints and 4 torso keypoints. The returned information will be\n","  used to determine the crop size. See determineCropRegion for more detail.\n","  \"\"\"\n","  torso_joints = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip']\n","  max_torso_yrange = 0.0\n","  max_torso_xrange = 0.0\n","  for joint in torso_joints:\n","    dist_y = abs(center_y - target_keypoints[joint][0])\n","    dist_x = abs(center_x - target_keypoints[joint][1])\n","    if dist_y > max_torso_yrange:\n","      max_torso_yrange = dist_y\n","    if dist_x > max_torso_xrange:\n","      max_torso_xrange = dist_x\n","\n","  max_body_yrange = 0.0\n","  max_body_xrange = 0.0\n","  for joint in KEYPOINT_DICT.keys():\n","    if keypoints[0, 0, KEYPOINT_DICT[joint], 2] < MIN_CROP_KEYPOINT_SCORE:\n","      continue\n","    dist_y = abs(center_y - target_keypoints[joint][0]);\n","    dist_x = abs(center_x - target_keypoints[joint][1]);\n","    if dist_y > max_body_yrange:\n","      max_body_yrange = dist_y\n","\n","    if dist_x > max_body_xrange:\n","      max_body_xrange = dist_x\n","\n","  return [max_torso_yrange, max_torso_xrange, max_body_yrange, max_body_xrange]\n","\n","def determine_crop_region(\n","      keypoints, image_height,\n","      image_width):\n","  \"\"\"Determines the region to crop the image for the model to run inference on.\n","\n","  The algorithm uses the detected joints from the previous frame to estimate\n","  the square region that encloses the full body of the target person and\n","  centers at the midpoint of two hip joints. The crop size is determined by\n","  the distances between each joints and the center point.\n","  When the model is not confident with the four torso joint predictions, the\n","  function returns a default crop which is the full image padded to square.\n","  \"\"\"\n","  target_keypoints = {}\n","  for joint in KEYPOINT_DICT.keys():\n","    target_keypoints[joint] = [\n","      keypoints[0, 0, KEYPOINT_DICT[joint], 0] * image_height,\n","      keypoints[0, 0, KEYPOINT_DICT[joint], 1] * image_width\n","    ]\n","\n","  if torso_visible(keypoints):\n","    center_y = (target_keypoints['left_hip'][0] +\n","                target_keypoints['right_hip'][0]) / 2;\n","    center_x = (target_keypoints['left_hip'][1] +\n","                target_keypoints['right_hip'][1]) / 2;\n","\n","    (max_torso_yrange, max_torso_xrange,\n","      max_body_yrange, max_body_xrange) = determine_torso_and_body_range(\n","          keypoints, target_keypoints, center_y, center_x)\n","\n","    crop_length_half = np.amax(\n","        [max_torso_xrange * 1.9, max_torso_yrange * 1.9,\n","          max_body_yrange * 1.2, max_body_xrange * 1.2])\n","\n","    tmp = np.array(\n","        [center_x, image_width - center_x, center_y, image_height - center_y])\n","    crop_length_half = np.amin(\n","        [crop_length_half, np.amax(tmp)]);\n","\n","    crop_corner = [center_y - crop_length_half, center_x - crop_length_half];\n","\n","    if crop_length_half > max(image_width, image_height) / 2:\n","      return init_crop_region(image_height, image_width)\n","    else:\n","      crop_length = crop_length_half * 2;\n","      return {\n","        'y_min': crop_corner[0] / image_height,\n","        'x_min': crop_corner[1] / image_width,\n","        'y_max': (crop_corner[0] + crop_length) / image_height,\n","        'x_max': (crop_corner[1] + crop_length) / image_width,\n","        'height': (crop_corner[0] + crop_length) / image_height -\n","            crop_corner[0] / image_height,\n","        'width': (crop_corner[1] + crop_length) / image_width -\n","            crop_corner[1] / image_width\n","      }\n","  else:\n","    return init_crop_region(image_height, image_width)\n","\n","def crop_and_resize(image, crop_region, crop_size):\n","  \"\"\"Crops and resize the image to prepare for the model input.\"\"\"\n","  boxes=[[crop_region['y_min'], crop_region['x_min'],\n","          crop_region['y_max'], crop_region['x_max']]]\n","  output_image = tf.image.crop_and_resize(\n","      image, box_indices=[0], boxes=boxes, crop_size=crop_size)\n","  return output_image\n","\n","def run_inference(movenet, image, crop_region, crop_size):\n","  \"\"\"Runs model inference on the cropped region.\n","\n","  The function runs the model inference on the cropped region and updates the\n","  model output to the original image coordinate system.\n","  \"\"\"\n","  image_height, image_width, _ = image.shape\n","  input_image = crop_and_resize(\n","    tf.expand_dims(image, axis=0), crop_region, crop_size=crop_size)\n","  # Run model inference.\n","  keypoints_with_scores = movenet(input_image)\n","  # Update the coordinates.\n","  for idx in range(17):\n","    keypoints_with_scores[0, 0, idx, 0] = (\n","        crop_region['y_min'] * image_height +\n","        crop_region['height'] * image_height *\n","        keypoints_with_scores[0, 0, idx, 0]) / image_height\n","    keypoints_with_scores[0, 0, idx, 1] = (\n","        crop_region['x_min'] * image_width +\n","        crop_region['width'] * image_width *\n","        keypoints_with_scores[0, 0, idx, 1]) / image_width\n","\n","  return keypoints_with_scores\n","\n","\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"S7PcKwTbx8MI","cellView":"form","executionInfo":{"status":"ok","timestamp":1722961856830,"user_tz":-60,"elapsed":7,"user":{"displayName":"MOHAMMED ALSIRAJI","userId":"11898263542769256225"}}},"outputs":[],"source":["#@title extract_keypoints\n","def body_visible(keypoints):\n","  \"\"\"Checks whether there are enough BODY keypoints.\n","\n","  This function checks whether the model is confident at predicting one of the\n","  ** shoulders / hips / knees **\n","  which is required to determine a good frame.\n","  \"\"\"\n","  KEYPOINT_SCORE = 0.3\n","  return (keypoints[0, 0, KEYPOINT_DICT['left_hip'], 2] >\n","           KEYPOINT_SCORE and\n","          keypoints[0, 0, KEYPOINT_DICT['right_hip'], 2] >\n","           KEYPOINT_SCORE and\n","          keypoints[0, 0, KEYPOINT_DICT['right_knee'], 2] >\n","           KEYPOINT_SCORE and\n","          keypoints[0, 0, KEYPOINT_DICT['left_knee'], 2] >\n","           KEYPOINT_SCORE and\n","          keypoints[0, 0, KEYPOINT_DICT['left_shoulder'], 2] >\n","           KEYPOINT_SCORE and\n","          keypoints[0, 0, KEYPOINT_DICT['right_shoulder'], 2] >\n","           KEYPOINT_SCORE)"]},{"cell_type":"code","source":["#@title MoveNet Lightning nn predict_video\n","\n","def predict_Fall_MVLight(mv_svm_model,VIDEO_IN,vid_num , mtype):\n","\n","  cap = cv2.VideoCapture(VIDEO_IN)\n","  fps = cap.get(cv2.CAP_PROP_FPS)\n","\n","  time_step = 1.0 / fps\n","  preds = []\n","  # Get frame width and height\n","  frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","  frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","  out_path = os.path.join(OUT_directory, f'MoveNetLightning_{mtype}_detected_video_{vid_num}.mp4')\n","  out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*'XVID'), fps, (frame_width, frame_height))\n","  vidoe_data = []\n","\n","  num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","  # Initialize a variable to keep track of the current timestamp\n","  current_timestamp = 0.0\n","  idx = 0\n","  bar = display(progress(0, num_frames-1), display_id=True)\n","\n","  crop_region = init_crop_region(frame_height, frame_width)\n","\n","  fall_detected = False\n","  fall_time = None\n","\n","  # Loop through each frame in the video using VideoCapture#read()\n","  while cap.isOpened():\n","      ret, frame = cap.read()\n","      if not ret:\n","          break\n","\n","      # Convert the frame received from OpenCV to a MediaPipe’s Image object.\n","      # Note: MediaPipe's Image object expects the data to be in RGB format.\n","      mpstart_time = time.time()\n","\n","      rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","      keypoints_with_scores = run_inference(\n","          movenet, rgb_frame, crop_region, crop_size=[input_size, input_size])\n","\n","      annotated_image = draw_prediction_on_image(\n","          rgb_frame.astype(np.uint8), keypoints_with_scores, crop_region=None,\n","          close_figure=True, output_image_height=frame_height)\n","\n","      crop_region = determine_crop_region(\n","          keypoints_with_scores, frame_height, frame_width)\n","\n","      #   MvLight_keypoint_columns\n","\n","      frame_data = {}\n","      for i in range(17):\n","          kpts_x = keypoints_with_scores[:, :, i, 1][0][0]\n","          kpts_y = keypoints_with_scores[:, :, i, 0][0][0]\n","          kpts_scores = keypoints_with_scores[:, :, i, 2][0][0]\n","          keypoint_name = list(KEYPOINT_DICT.keys())[list(KEYPOINT_DICT.values()).index(i)]\n","          frame_data[f'MoveNetLightning_{keypoint_name}'] = [kpts_x, kpts_y]\n","\n","      # PREDICTION SHOULD HAPPEN HERE USING frame_data\n","      prediction = mv_svm_model.predict(prepareForPrediction(frame_data , MvLight_keypoint_columns))\n","      # print('prediction:: ',prediction)\n","\n","      label = 'Fall' if prediction[0][0] > 0.7 else 'Not Fall'\n","      # print(prediction)\n","      preds.append(prediction[0][0])\n","      # label = 'PREDECTING'#predictions[0]\n","      cv2.putText(annotated_image,f'{label}: {(prediction[0][0]*100):.2f}%', (10, 30),\n","          cv2.FONT_HERSHEY_SIMPLEX, 1,(0, 255, 0) if label == 'Not Fall' else (0, 0, 255), 2)\n","\n","\n","      title_text = 'Pose by MoveNet Lightning'\n","      text_size, _ = cv2.getTextSize(title_text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n","      text_width, text_height = text_size\n","      cv2.putText(annotated_image, title_text, (frame_width - 350, 30),\n","          cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n","\n","      timestamp_text = f'Time: {current_timestamp:.2f}s'\n","      cv2.putText(annotated_image, timestamp_text, (30, frame_height - 20),\n","          cv2.FONT_HERSHEY_SIMPLEX, 1,(0, 255, 0) if label == 'Not Fall' else (0, 0, 255), 2)\n","\n","      if label == 'Fall' and not fall_detected:\n","          fall_time = current_timestamp\n","          fall_detected = True\n","\n","      # Display the fall detection time if a fall was detected\n","      if fall_detected:\n","          fall_time_text = f'Fall detected at: {fall_time:.2f} s'\n","          cv2.putText(annotated_image, fall_time_text, (10, 100),\n","                      cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n","\n","\n","\n","      # print('MoveNet annotated_image :', type(annotated_image) , annotated_image.shape )\n","      out.write(annotated_image)\n","      # cv2_imshow(annotated_image)\n","\n","      mpend_time = time.time()\n","      mpexecution_time = (mpend_time - mpstart_time) * 1000  # convert to milliseconds\n","\n","      mp_vid = {\n","      'Video_num':vid_num,\n","      'frame_indx':idx ,\n","      'predictions':prediction[0][0],\n","      'frame_proccessing_duration':mpexecution_time,\n","      'fall_detection_duration_perVideo':fall_time ,\n","      'frame_width': frame_width,\n","      'frame_height': frame_height\n","      }\n","\n","      vidoe_data.append(mp_vid)\n","\n","\n","      current_timestamp += time_step\n","      # print(f\"Frame: {idx} / {num_frames}\")\n","      bar.update(progress(idx, num_frames-1))\n","      idx += 1\n","      if cv2.waitKey(int(1000/fps)) & 0xFF == ord('q'):\n","          break\n","  vidoe_datadf = pd.DataFrame(vidoe_data)\n","  csv_path = os.path.join(OUT_directory,f'MoveNetLightning_{mtype}_results_vid{vid_num}.csv')\n","  vidoe_datadf.to_csv(csv_path, index=False)\n","\n","\n","  cap.release()\n","  out.release()\n","  cv2.destroyAllWindows()\n","  return preds , fall_time"],"metadata":{"cellView":"form","id":"zsPhISL6fELk","executionInfo":{"status":"ok","timestamp":1722961856830,"user_tz":-60,"elapsed":7,"user":{"displayName":"MOHAMMED ALSIRAJI","userId":"11898263542769256225"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["#@title MoveNet Lightning svm predict_video\n","\n","MvLight_keypoint_columns = ['MoveNetLightning_nose', 'MoveNetLightning_left_eye', 'MoveNetLightning_right_eye',\n","                    'MoveNetLightning_left_ear', 'MoveNetLightning_right_ear',\n","                    'MoveNetLightning_left_shoulder', 'MoveNetLightning_right_shoulder',\n","                    'MoveNetLightning_left_elbow', 'MoveNetLightning_right_elbow',\n","                    'MoveNetLightning_left_wrist', 'MoveNetLightning_right_wrist',\n","                    'MoveNetLightning_left_hip', 'MoveNetLightning_right_hip',\n","                    'MoveNetLightning_left_knee', 'MoveNetLightning_right_knee',\n","                    'MoveNetLightning_left_ankle', 'MoveNetLightning_right_ankle']\n","\n","\n","def predict_Fall_SVM_MVLight(mv_svm_model,VIDEO_IN,vid_num):\n","\n","  cap = cv2.VideoCapture(VIDEO_IN)\n","  fps = cap.get(cv2.CAP_PROP_FPS)\n","\n","  time_step = 1.0 / fps\n","  preds = []\n","  # Get frame width and height\n","  frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","  frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","  out_path = os.path.join(OUT_directory, f'MoveNetLightning_SVM_detected_video_{vid_num}.mp4')\n","  out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*'XVID'), fps, (frame_width, frame_height))\n","  vidoe_data = []\n","\n","  num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","  # Initialize a variable to keep track of the current timestamp\n","  current_timestamp = 0.0\n","  idx = 0\n","  bar = display(progress(0, num_frames-1), display_id=True)\n","\n","  crop_region = init_crop_region(frame_height, frame_width)\n","\n","  fall_detected = False\n","  fall_time = None\n","\n","  # Loop through each frame in the video using VideoCapture#read()\n","  while cap.isOpened():\n","      ret, frame = cap.read()\n","      if not ret:\n","          break\n","\n","      # Convert the frame received from OpenCV to a MediaPipe’s Image object.\n","      # Note: MediaPipe's Image object expects the data to be in RGB format.\n","      mpstart_time = time.time()\n","\n","      rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","      keypoints_with_scores = run_inference(\n","          movenet, rgb_frame, crop_region, crop_size=[input_size, input_size])\n","\n","      annotated_image = draw_prediction_on_image(\n","          rgb_frame.astype(np.uint8), keypoints_with_scores, crop_region=None,\n","          close_figure=True, output_image_height=frame_height)\n","\n","      crop_region = determine_crop_region(\n","          keypoints_with_scores, frame_height, frame_width)\n","\n","      #   MvLight_keypoint_columns\n","\n","      frame_data = {}\n","      for i in range(17):\n","          kpts_x = keypoints_with_scores[:, :, i, 1][0][0]\n","          kpts_y = keypoints_with_scores[:, :, i, 0][0][0]\n","          kpts_scores = keypoints_with_scores[:, :, i, 2][0][0]\n","          keypoint_name = list(KEYPOINT_DICT.keys())[list(KEYPOINT_DICT.values()).index(i)]\n","          frame_data[f'MoveNetLightning_{keypoint_name}'] = [kpts_x, kpts_y, kpts_scores]\n","\n","      # PREDICTION SHOULD HAPPEN HERE USING frame_data\n","      prediction = mv_svm_model.predict(prepareForSVMPrediction(frame_data , MvLight_keypoint_columns))\n","      label = 'Fall' if prediction[0] == 1 else 'Not Fall'\n","      preds.append(prediction[0])\n","      cv2.putText(annotated_image,f'{label}: {prediction}', (10, 30),\n","          cv2.FONT_HERSHEY_SIMPLEX, 1,(0, 255, 0) if label == 'Not Fall' else (0, 0, 255), 2)\n","\n","\n","      title_text = 'Pose by MoveNet Lightning'\n","      text_size, _ = cv2.getTextSize(title_text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n","      text_width, text_height = text_size\n","      cv2.putText(annotated_image, title_text, (frame_width - 350, 30),\n","          cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n","\n","      timestamp_text = f'Time: {current_timestamp:.2f}s'\n","      cv2.putText(annotated_image, timestamp_text, (30, frame_height - 20),\n","          cv2.FONT_HERSHEY_SIMPLEX, 1,(0, 255, 0) if label == 'Not Fall' else (0, 0, 255), 2)\n","\n","      if label == 'Fall' and not fall_detected:\n","          fall_time = current_timestamp\n","          fall_detected = True\n","\n","      # Display the fall detection time if a fall was detected\n","      if fall_detected:\n","          fall_time_text = f'Fall detected at: {fall_time:.2f} s'\n","          cv2.putText(annotated_image, fall_time_text, (10, 100),\n","                      cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n","\n","\n","\n","      # print('MoveNet annotated_image :', type(annotated_image) , annotated_image.shape )\n","      out.write(annotated_image)\n","      # cv2_imshow(annotated_image)\n","\n","      mpend_time = time.time()\n","      mpexecution_time = (mpend_time - mpstart_time) * 1000  # convert to milliseconds\n","\n","      mp_vid = {\n","      'Video_num':vid_num,\n","      'frame_indx':idx ,\n","      'predictions':prediction[0],\n","      'frame_proccessing_duration':mpexecution_time,\n","      'fall_detection_duration_perVideo':fall_time ,\n","      'frame_width': frame_width,\n","      'frame_height': frame_height\n","      }\n","\n","      vidoe_data.append(mp_vid)\n","\n","\n","      current_timestamp += time_step\n","      # print(f\"Frame: {idx} / {num_frames}\")\n","      bar.update(progress(idx, num_frames-1))\n","      idx += 1\n","      if cv2.waitKey(int(1000/fps)) & 0xFF == ord('q'):\n","          break\n","  vidoe_datadf = pd.DataFrame(vidoe_data)\n","  csv_path = os.path.join(OUT_directory,f'MoveNetLightning_SVM_results_vid{vid_num}.csv')\n","  vidoe_datadf.to_csv(csv_path, index=False)\n","\n","\n","  cap.release()\n","  out.release()\n","  cv2.destroyAllWindows()\n","  return preds , fall_time"],"metadata":{"id":"VokuNcT0RfhZ","cellView":"form","executionInfo":{"status":"ok","timestamp":1722961856830,"user_tz":-60,"elapsed":6,"user":{"displayName":"MOHAMMED ALSIRAJI","userId":"11898263542769256225"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["#Reformat Vlidation Set from images to videos"],"metadata":{"id":"sU32QPh7PaTT"}},{"cell_type":"code","source":["def make_video_from_frames(frames_folder, output_video):\n","    # Get all PNG files in the frames folder\n","    frame_files = sorted([f for f in os.listdir(frames_folder) if f.endswith('.png')])\n","\n","    # Determine the width and height from the first image\n","    frame = cv2.imread(os.path.join(frames_folder, frame_files[0]))\n","    height, width, layers = frame.shape\n","    # Define the codec and create VideoWriter object\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Be sure to use lower case\n","    out = cv2.VideoWriter(output_video, fourcc, 30.0, (width, height))\n","    for i, f in enumerate(frame_files):\n","        filename = os.path.join(frames_folder, f)\n","\n","        img = cv2.imread(filename)\n","        out.write(img)  # Write out frame to video\n","    print(f\"Processes: {filename})\")\n","    # Release everything if job is finished\n","    out.release()\n","    cv2.destroyAllWindows()"],"metadata":{"id":"R9f-jKDnPZoe","executionInfo":{"status":"ok","timestamp":1722961856830,"user_tz":-60,"elapsed":6,"user":{"displayName":"MOHAMMED ALSIRAJI","userId":"11898263542769256225"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["CREATE_VALID_SET = False\n","if CREATE_VALID_SET:\n","  for i in range(1,31):\n","    frames_folder = f'/content/drive/MyDrive/AI-RDP/VALIDATION_SET/fall-{i}-cam0-rgb'\n","    output_video = f'/content/drive/MyDrive/AI-RDP/VALIDATION_SET/fall{i}.mp4'\n","    make_video_from_frames(frames_folder, output_video)"],"metadata":{"id":"1BLN6n4ePrxV","executionInfo":{"status":"ok","timestamp":1722961856830,"user_tz":-60,"elapsed":6,"user":{"displayName":"MOHAMMED ALSIRAJI","userId":"11898263542769256225"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["#Prepare Prediction Models\n","Check that the models directories are correct here"],"metadata":{"id":"hlOeYpBCg7xb"}},{"cell_type":"code","source":["from joblib import load\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.models import load_model\n","import time\n","\n","mp_model_path = '/content/drive/MyDrive/AI-RDP/DEMO/MediaPipe_NN_model50Epochsv2.keras'\n","mp_mlp_model = load_model(mp_model_path)\n","\n","mv_model_path = '/content/drive/MyDrive/AI-RDP/DEMO/MoveNet_NN_model50Epochsv2.keras'\n","mv_mlp_model = load_model(mv_model_path)\n","print(\"MLP Models loaded successfully.\")\n","\n","\n","# Load the LSTM models\n","mp_lstm_model_path = '/content/drive/MyDrive/AI-RDP/DEMO/lstm_fall_detection_mpv2.h5'\n","mv_lstm_model_path = '/content/drive/MyDrive/AI-RDP/DEMO/lstm_fall_detection_mvv2.h5'\n","\n","mp_lstm_model = load_model(mp_lstm_model_path)\n","mv_lstm_model = load_model(mv_lstm_model_path)\n","\n","print(\"LSTM models loaded successfully.\")\n","\n","mp_model_path = '/content/drive/MyDrive/AI-RDP/DEMO/svm_MediaPipe_model.joblib'\n","mp_svm_model = load(mp_model_path)\n","mv_model_path = '/content/drive/MyDrive/AI-RDP/DEMO/svm_lightning_model.joblib'\n","mv_svm_model = load(mv_model_path)\n","print(\"Models loaded successfully.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LD-pNearKm5Z","executionInfo":{"status":"ok","timestamp":1722961862305,"user_tz":-60,"elapsed":5481,"user":{"displayName":"MOHAMMED ALSIRAJI","userId":"11898263542769256225"}},"outputId":"a98d6b49-6abc-4324-87ae-2d5203efdb40"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["MLP Models loaded successfully.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["LSTM models loaded successfully.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.2.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Models loaded successfully.\n"]}]},{"cell_type":"markdown","source":["#Predict the entire Validation Set"],"metadata":{"id":"gqKvTqBROBr0"}},{"cell_type":"code","source":["TEST_Validation = False\n","if TEST_Validation:\n","  mp_data = []\n","  mv_data = []\n","\n","  for i in range(1,31):\n","    vid_num = i\n","    VIDPATH = f'/content/drive/MyDrive/AI-RDP/VALIDATION_SET/fall{vid_num}.mp4'\n","    MP_PREDS , mpstep= predict_Fall_SVM_MP(mp_svm_model,VIDPATH,vid_num)\n","    MV_PREDS , mvstep = predict_Fall_SVM_MVLight(mv_svm_model,VIDPATH,vid_num)\n","    print('Proccessed: ',VIDPATH)\n","\n","  for i in range(1,31):\n","    vid_num = i\n","    VIDPATH = f'/content/drive/MyDrive/AI-RDP/VALIDATION_SET/fall{vid_num}.mp4'\n","    MP_PREDS , mpstep= predict_Fall_MP(mp_mlp_model,VIDPATH,vid_num,'mlp')\n","    MV_PREDS , mvstep = predict_Fall_MVLight(mv_mlp_model,VIDPATH,vid_num,'mlp')\n","    print('Proccessed: ',VIDPATH)\n","  for i in range(1,31):\n","    vid_num = i\n","    VIDPATH = f'/content/drive/MyDrive/AI-RDP/VALIDATION_SET/fall{vid_num}.mp4'\n","    MP_PREDS , mpstep= predict_Fall_MP(mp_lstm_model,VIDPATH,vid_num,'lstm')\n","    MV_PREDS , mvstep = predict_Fall_MVLight(mv_lstm_model,VIDPATH,vid_num,'lstm')\n","    print('Proccessed: ',VIDPATH)\n"],"metadata":{"id":"D_WXcnBHOBA5","executionInfo":{"status":"ok","timestamp":1722961862305,"user_tz":-60,"elapsed":6,"user":{"displayName":"MOHAMMED ALSIRAJI","userId":"11898263542769256225"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["# Create Demo Videos"],"metadata":{"id":"Ly0FYACghDSf"}},{"cell_type":"code","source":["#@title createDemoVideo\n","def createDemoVideo(vid_num , VIDPATH , TEST , mv_model , mp_model , mtype):\n","  if TEST:\n","    if mtype == 'svm':\n","      mpstart_time = time.time()\n","      MP_PREDS , mpstep= predict_Fall_SVM_MP(mp_model,VIDPATH,vid_num)\n","      mpend_time = time.time()\n","      mpexecution_time = (mpend_time - mpstart_time) * 1000  # convert to milliseconds\n","\n","      mvstart_time = time.time()\n","      MV_PREDS , mvstep = predict_Fall_SVM_MVLight(mv_model,VIDPATH,vid_num)\n","      mvend_time = time.time()\n","      mvexecution_time = (mvend_time - mvstart_time) * 1000  # convert to milliseconds\n","    else:\n","      mpstart_time = time.time()\n","      MP_PREDS , mpstep= predict_Fall_MP(mp_model,VIDPATH,vid_num , mtype)\n","      mpend_time = time.time()\n","      mpexecution_time = (mpend_time - mpstart_time) * 1000  # convert to milliseconds\n","      mvstart_time = time.time()\n","      MV_PREDS , mvstep = predict_Fall_MVLight(mv_model,VIDPATH,vid_num , mtype)\n","      mvend_time = time.time()\n","      mvexecution_time = (mvend_time - mvstart_time) * 1000  # convert to milliseconds"],"metadata":{"id":"n9uFobCGPE0Y","executionInfo":{"status":"ok","timestamp":1722961862305,"user_tz":-60,"elapsed":5,"user":{"displayName":"MOHAMMED ALSIRAJI","userId":"11898263542769256225"}},"cellView":"form"},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["### Main Part to Run\n","Set the path to the video to process\n","\n","The output directory to save the files."],"metadata":{"id":"KjeGF0Lhj8Nn"}},{"cell_type":"code","source":["OUT_directory = '/content/drive/MyDrive/AI-RDP/DEMO'\n","VIDPATH = f'/content/drive/MyDrive/AI-RDP/DEMO/demo_video.mp4' #The path for the video to be processed\n","\n","TEST = True\n","vid_num = 44\n","\n","createDemoVideo(vid_num , VIDPATH , TEST , mv_svm_model , mp_svm_model , 'svm')\n","\n","createDemoVideo(vid_num , VIDPATH , TEST , mv_mlp_model , mp_mlp_model , 'mlp')\n","\n","createDemoVideo(vid_num , VIDPATH , TEST , mv_lstm_model , mp_lstm_model , 'lstm')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"k7yxYib9b5wb","executionInfo":{"status":"ok","timestamp":1722962066198,"user_tz":-60,"elapsed":203287,"user":{"displayName":"MOHAMMED ALSIRAJI","userId":"11898263542769256225"}},"outputId":"b41e42b2-b793-4f5d-8882-c29fa17846d9"},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","      <progress\n","          value='98'\n","          max='98',\n","          style='width: 100%'\n","      >\n","          98\n","      </progress>\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","      <progress\n","          value='98'\n","          max='98',\n","          style='width: 100%'\n","      >\n","          98\n","      </progress>\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","      <progress\n","          value='98'\n","          max='98',\n","          style='width: 100%'\n","      >\n","          98\n","      </progress>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","      <progress\n","          value='98'\n","          max='98',\n","          style='width: 100%'\n","      >\n","          98\n","      </progress>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","      <progress\n","          value='98'\n","          max='98',\n","          style='width: 100%'\n","      >\n","          98\n","      </progress>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","      <progress\n","          value='98'\n","          max='98',\n","          style='width: 100%'\n","      >\n","          98\n","      </progress>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"mTdIy6Jrsert","executionInfo":{"status":"ok","timestamp":1722962066199,"user_tz":-60,"elapsed":24,"user":{"displayName":"MOHAMMED ALSIRAJI","userId":"11898263542769256225"}}},"execution_count":19,"outputs":[]}]}